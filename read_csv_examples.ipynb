{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a5effc2",
   "metadata": {},
   "source": [
    "# Reading CSV Files Line by Line in Python\n",
    "\n",
    "This notebook demonstrates different methods to read and process CSV files line by line in Python. We'll cover various approaches from basic file operations to more sophisticated methods using pandas, suitable for different file sizes and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d62ff",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import the necessary libraries we'll use throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0df29a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from typing import Iterator, List, Dict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114d0932",
   "metadata": {},
   "source": [
    "## 2. Read CSV File Using csv Module\n",
    "\n",
    "The `csv` module is Python's built-in solution for reading CSV files. It handles different CSV formats and provides a simple interface for reading files line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b9d4712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: csv.reader\n",
      "Headers: ['date', 'symbol', 'price', 'shares']\n",
      "Processing trade: 100 shares of AAPL @ $190.50 on 2025-01-01\n",
      "Processing trade: 50 shares of MSFT @ $375.25 on 2025-01-02\n",
      "Processing trade: 75 shares of GOOGL @ $140.75 on 2025-01-03\n",
      "\n",
      "Method 2: csv.DictReader\n",
      "Processing trade: 100 shares of AAPL @ $190.50 on 2025-01-01\n",
      "Processing trade: 50 shares of MSFT @ $375.25 on 2025-01-02\n",
      "Processing trade: 75 shares of GOOGL @ $140.75 on 2025-01-03\n"
     ]
    }
   ],
   "source": [
    "# Example CSV content\n",
    "sample_csv = \"\"\"date,symbol,price,shares\n",
    "2025-01-01,AAPL,190.50,100\n",
    "2025-01-02,MSFT,375.25,50\n",
    "2025-01-03,GOOGL,140.75,75\"\"\"\n",
    "\n",
    "# Create a CSV file-like object\n",
    "csv_file = StringIO(sample_csv)\n",
    "\n",
    "# Method 1: Using csv.reader\n",
    "print(\"Method 1: csv.reader\")\n",
    "csv_file.seek(0)  # Reset file pointer to start\n",
    "csv_reader = csv.reader(csv_file)\n",
    "header = next(csv_reader)  # Read header row\n",
    "print(f\"Headers: {header}\")\n",
    "\n",
    "for row in csv_reader:\n",
    "    date, symbol, price, shares = row\n",
    "    print(f\"Processing trade: {shares} shares of {symbol} @ ${price} on {date}\")\n",
    "\n",
    "# Method 2: Using csv.DictReader\n",
    "print(\"\\nMethod 2: csv.DictReader\")\n",
    "csv_file.seek(0)  # Reset file pointer\n",
    "dict_reader = csv.DictReader(csv_file)\n",
    "\n",
    "for row in dict_reader:\n",
    "    print(f\"Processing trade: {row['shares']} shares of {row['symbol']} @ ${row['price']} on {row['date']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc389e",
   "metadata": {},
   "source": [
    "## 3. Read CSV with File Object\n",
    "\n",
    "For simple CSV files, you can also use basic file operations. This approach is useful when you need maximum control over the parsing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eb5fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset our sample file\n",
    "csv_file.seek(0)\n",
    "\n",
    "# Read line by line with basic file operations\n",
    "header = csv_file.readline().strip()  # Read and store header\n",
    "print(f\"Headers: {header}\")\n",
    "\n",
    "for line in csv_file:\n",
    "    # Skip empty lines\n",
    "    if not line.strip():\n",
    "        continue\n",
    "        \n",
    "    # Parse the line (split by comma and strip whitespace)\n",
    "    date, symbol, price, shares = [field.strip() for field in line.split(',')]\n",
    "    print(f\"Processing trade: {shares} shares of {symbol} @ ${price} on {date}\")\n",
    "\n",
    "# Example of a generator function for memory-efficient reading\n",
    "def read_csv_generator(file_obj, skip_header=True):\n",
    "    if skip_header:\n",
    "        next(file_obj)  # Skip header row\n",
    "    for line in file_obj:\n",
    "        if line.strip():  # Skip empty lines\n",
    "            yield [field.strip() for field in line.split(',')]\n",
    "\n",
    "# Use the generator\n",
    "print(\"\\nUsing generator function:\")\n",
    "csv_file.seek(0)  # Reset file pointer\n",
    "for row in read_csv_generator(csv_file):\n",
    "    date, symbol, price, shares = row\n",
    "    print(f\"Processing trade: {shares} shares of {symbol} @ ${price} on {date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c576ad",
   "metadata": {},
   "source": [
    "## 4. Process CSV with Pandas\n",
    "\n",
    "Pandas provides powerful tools for reading CSV files. While it typically loads the entire file into memory, you can use the `chunksize` parameter to read large files in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d049d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full DataFrame:\n",
      "         date symbol   price  shares\n",
      "0  2025-01-01   AAPL  190.50     100\n",
      "1  2025-01-02   MSFT  375.25      50\n",
      "2  2025-01-03  GOOGL  140.75      75\n",
      "\n",
      "Processing with iterrows():\n",
      "Processing trade: 100 shares of AAPL @ $190.5 on 2025-01-01\n",
      "Processing trade: 50 shares of MSFT @ $375.25 on 2025-01-02\n",
      "Processing trade: 75 shares of GOOGL @ $140.75 on 2025-01-03\n",
      "\n",
      "Processing with chunks:\n",
      "\n",
      "Processing chunk:\n",
      "         date symbol   price  shares\n",
      "0  2025-01-01   AAPL  190.50     100\n",
      "1  2025-01-02   MSFT  375.25      50\n",
      "\n",
      "Processing chunk:\n",
      "         date symbol   price  shares\n",
      "2  2025-01-03  GOOGL  140.75      75\n"
     ]
    }
   ],
   "source": [
    "# Read the entire CSV into a DataFrame\n",
    "csv_file.seek(0)\n",
    "df = pd.read_csv(csv_file)\n",
    "print(\"Full DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Process row by row using iterrows()\n",
    "print(\"\\nProcessing with iterrows():\")\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Processing trade: {row['shares']} shares of {row['symbol']} @ ${row['price']} on {row['date']}\")\n",
    "\n",
    "# Read in chunks (useful for large files)\n",
    "print(\"\\nProcessing with chunks:\")\n",
    "csv_file.seek(0)\n",
    "chunk_size = 2  # Small chunk size for demonstration\n",
    "for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "    print(\"\\nProcessing chunk:\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c9f07",
   "metadata": {},
   "source": [
    "## 5. Handle Large CSV Files\n",
    "\n",
    "When working with large CSV files, memory efficiency becomes crucial. Here are some techniques for processing large files without loading them entirely into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf967a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient CSV reader with generator\n",
    "def read_csv_efficient(file_obj, chunk_size=1000):\n",
    "    \"\"\"Read a CSV file in chunks using a generator.\"\"\"\n",
    "    reader = csv.reader(file_obj)\n",
    "    header = next(reader)  # Get header row\n",
    "    \n",
    "    # Create chunks using itertools.islice\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(reader, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "# Example usage with our sample data\n",
    "print(\"Processing large file in chunks:\")\n",
    "csv_file.seek(0)\n",
    "next(csv_file)  # Skip header since read_csv_efficient handles it\n",
    "\n",
    "for chunk in read_csv_efficient(csv_file, chunk_size=2):\n",
    "    print(\"\\nProcessing chunk:\")\n",
    "    for row in chunk:\n",
    "        date, symbol, price, shares = row\n",
    "        print(f\"Processing trade: {shares} shares of {symbol} @ ${price} on {date}\")\n",
    "\n",
    "# Using pandas for large files with specific columns\n",
    "print(\"\\nReading specific columns with pandas:\")\n",
    "csv_file.seek(0)\n",
    "selected_columns = ['date', 'symbol']  # Only read these columns\n",
    "for chunk in pd.read_csv(csv_file, \n",
    "                        usecols=selected_columns,\n",
    "                        chunksize=2):\n",
    "    print(\"\\nChunk with selected columns:\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b3aec",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We've covered several methods to read CSV files line by line:\n",
    "\n",
    "1. Using `csv.reader` and `csv.DictReader` from the built-in `csv` module\n",
    "2. Basic file operations with `split()` and generators\n",
    "3. Pandas methods including `iterrows()` and chunked reading\n",
    "4. Memory-efficient techniques for large files\n",
    "\n",
    "Choose the method that best fits your needs:\n",
    "- Use `csv` module for simple CSV processing\n",
    "- Use basic file operations for maximum control\n",
    "- Use pandas for data analysis and transformation\n",
    "- Use generators and chunks for large files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
